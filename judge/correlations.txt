
 {'name': 'hf-llama2-truth', 'files_name': 'truthfulqa-truth-judge-llama2-7B', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 hf-llama2-truth 	 0.77	0.68	0.38	0.19	0.54
Meta-Llama 	 hf-llama2-truth 	 0.74	0.68	0.63	0.19	0.59
gemma-2-27 	 hf-llama2-truth 	 0.49	0.48	0.55	0.29	0.46
Average per language		 	 0.67	0.61	0.52	0.22	0.53
Errors in the labels: 0

 {'name': 'new-llama3-truth', 'files_name': 'llama3-1_7B_truth_judge_final', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 new-llama3-truth 	 0.76	0.7	0.61	0.3	0.64
Meta-Llama 	 new-llama3-truth 	 0.67	0.64	0.66	0.46	0.58
gemma-2-27 	 new-llama3-truth 	 0.6	0.47	0.62	0.55	0.55
Average per language		 	 0.68	0.6	0.63	0.44	0.59
Errors in the labels: 0

 {'name': 'llama3.1-truth', 'files_name': 'llama3.1.2_truth_judge', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 llama3.1-truth 	 0.79	0.67	0.56	0.24	0.6
Meta-Llama 	 llama3.1-truth 	 0.62	0.65	0.69	0.44	0.66
gemma-2-27 	 llama3.1-truth 	 0.59	0.51	0.57	0.43	0.58
Average per language		 	 0.66	0.61	0.61	0.37	0.61
Errors in the labels: 0

 {'name': 'gemma9b\t', 'files_name': 'gemma9b_truth_judge', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 gemma9b	 	 0.79	0.62	0.67	0.42	0.58
Meta-Llama 	 gemma9b	 	 0.7	0.66	0.69	0.56	0.64
gemma-2-27 	 gemma9b	 	 0.46	0.53	0.58	0.41	0.58
Average per language		 	 0.65	0.6	0.65	0.46	0.6
Errors in the labels: 0

 {'name': '2-llama3.1-instruct', 'files_name': '2-llama3.1_instruct_truth_judge', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 2-llama3.1-instruct 	 0.72	0.65	0.68	0.34	0.6
Meta-Llama 	 2-llama3.1-instruct 	 0.75	0.68	0.69	0.4	0.75
gemma-2-27 	 2-llama3.1-instruct 	 0.6	0.63	0.59	0.52	0.53
Average per language		 	 0.69	0.65	0.65	0.42	0.63
Errors in the labels: 0

 {'name': 'multi-ll3.1-truth', 'files_name': 'llama3.1_multi_truth_judge', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 multi-ll3.1-truth 	 0.75	0.61	0.61	0.5	0.66
Meta-Llama 	 multi-ll3.1-truth 	 0.68	0.64	0.72	0.6	0.73
gemma-2-27 	 multi-ll3.1-truth 	 0.53	0.55	0.62	0.48	0.53
Average per language		 	 0.65	0.6	0.65	0.53	0.64
Errors in the labels: 0

 {'name': 'multillama3-truth', 'files_name': 'llama-3-multi_truth_judge', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 multillama3-truth 	 0.75	0.56	0.67	0.5	0.64
Meta-Llama 	 multillama3-truth 	 0.68	0.7	0.78	0.62	0.73
gemma-2-27 	 multillama3-truth 	 0.41	0.52	0.62	0.49	0.51
Average per language		 	 0.61	0.59	0.69	0.54	0.63
Errors in the labels: 0

 {'name': 'multi-gemma9b-truth', 'files_name': 'gemma9b_multi_truth_judge', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 multi-gemma9b-truth 	 0.72	0.6	0.61	0.41	0.68
Meta-Llama 	 multi-gemma9b-truth 	 0.65	0.73	0.7	0.62	0.81
gemma-2-27 	 multi-gemma9b-truth 	 0.51	0.56	0.54	0.47	0.59
Average per language		 	 0.63	0.63	0.62	0.5	0.69
Errors in the labels: 0

Average per model:
hf-llama2-truth 0.51
new-llama3-truth 0.59
llama3.1-truth 0.57
gemma9b	 0.59
2-llama3.1-instruct 0.61
multi-ll3.1-truth 0.62
multillama3-truth 0.61
multi-gemma9b-truth 0.62
