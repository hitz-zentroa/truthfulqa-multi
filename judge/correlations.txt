
 {'name': 'hf-llama2-truth', 'files_name': 'truthfulqa-truth-judge-llama2-7B', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 hf-llama2-truth 	 0.77	0.68	0.38	0.19	0.54
Meta-Llama 	 hf-llama2-truth 	 0.74	0.68	0.63	0.19	0.59
gemma-2-27 	 hf-llama2-truth 	 0.49	0.48	0.55	0.29	0.46
Meta-Llama 	 hf-llama2-truth 	 0.85	0.77	0.84	0.12	0.64
Average per language		 	 0.71	0.65	0.6	0.2	0.56
Errors in the labels: 0

 {'name': 'hf-llama2-info', 'files_name': 'truthfulqa-info-judge-llama2-7B', 'label': 'info'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 hf-llama2-info 	 0.49	0.27	0.19	0.24	0.26
Meta-Llama 	 hf-llama2-info 	 yes	yes	-0.02	0.08	yes
gemma-2-27 	 hf-llama2-info 	 0.17	0.15	0.16	0.09	0.17
Meta-Llama 	 hf-llama2-info 	 0.48	0.75	0.61	0.03	0.58
Average per language		 	 0.38	0.39	0.23	0.11	0.34
Errors in the labels: 0

 {'name': 'new-llama3-truth', 'files_name': 'llama3-1_7B_truth_judge_final', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 new-llama3-truth 	 0.76	0.7	0.61	0.3	0.64
Meta-Llama 	 new-llama3-truth 	 0.67	0.64	0.66	0.46	0.58
gemma-2-27 	 new-llama3-truth 	 0.6	0.47	0.62	0.55	0.55
Meta-Llama 	 new-llama3-truth 	 -	-	-	-	-
Average per language		 	 0.68	0.6	0.63	0.44	0.59
Errors in the labels: 0

 {'name': 'new-llama3-info', 'files_name': 'llama-3-info-new', 'label': 'info'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 new-llama3-info 	 0.66	yes	0.32	0.11	0.09
Meta-Llama 	 new-llama3-info 	 yes	yes	yes	0.15	-0.02
gemma-2-27 	 new-llama3-info 	 yes	0.15	0.27	0.21	-0.02
Meta-Llama 	 new-llama3-info 	 -	-	-	-	-
Average per language		 	 0.66	0.15	0.3	0.16	0.02
Errors in the labels: 0

 {'name': 'llama3.1-truth', 'files_name': 'llama3.1.2_truth_judge', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 llama3.1-truth 	 0.79	0.67	0.56	0.24	0.6
Meta-Llama 	 llama3.1-truth 	 0.62	0.65	0.69	0.44	0.66
gemma-2-27 	 llama3.1-truth 	 0.59	0.51	0.57	0.43	0.58
Meta-Llama 	 llama3.1-truth 	 -	-	-	-	-
Average per language		 	 0.66	0.61	0.61	0.37	0.61
Errors in the labels: 0

 {'name': 'gemma9b\t', 'files_name': 'gemma9b_truth_judge', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 gemma9b	 	 0.79	0.62	0.67	0.42	0.58
Meta-Llama 	 gemma9b	 	 0.7	0.66	0.69	0.56	0.64
gemma-2-27 	 gemma9b	 	 0.46	0.53	0.58	0.41	0.58
Meta-Llama 	 gemma9b	 	 -	-	-	-	-
Average per language		 	 0.65	0.6	0.65	0.46	0.6
Errors in the labels: 0

 {'name': 'llama3.1-instruct', 'files_name': '2-llama3.1_instruct_truth_judge', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 llama3.1-instruct 	 0.77	0.71	0.63	0.33	0.56
Meta-Llama 	 llama3.1-instruct 	 0.72	0.68	0.71	0.44	0.79
gemma-2-27 	 llama3.1-instruct 	 0.6	0.6	0.56	0.47	0.59
Meta-Llama 	 llama3.1-instruct 	 -	-	-	-	-
Average per language		 	 0.69	0.66	0.63	0.41	0.65
Errors in the labels: 0

 {'name': 'multi-llama3.1-inst', 'files_name': 'multi_llama3.1_instruct_truth_judge', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 multi-llama3.1-inst 	 0.72	0.62	0.59	0.45	0.68
Meta-Llama 	 multi-llama3.1-inst 	 0.69	0.67	0.76	0.64	0.79
gemma-2-27 	 multi-llama3.1-inst 	 0.54	0.57	0.6	0.57	0.66
Meta-Llama 	 multi-llama3.1-inst 	 0.87	0.88	0.92	0.7	0.77
Average per language		 	 0.71	0.68	0.72	0.59	0.72
Errors in the labels: 0

 {'name': 'multi-ll3.1-truth', 'files_name': 'llama3.1_multi_truth_judge', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 multi-ll3.1-truth 	 0.75	0.61	0.61	0.5	0.66
Meta-Llama 	 multi-ll3.1-truth 	 0.68	0.64	0.72	0.6	0.73
gemma-2-27 	 multi-ll3.1-truth 	 0.53	0.55	0.62	0.48	0.53
Meta-Llama 	 multi-ll3.1-truth 	 -	-	-	-	-
Average per language		 	 0.65	0.6	0.65	0.53	0.64
Errors in the labels: 0

 {'name': 'multillama3-truth', 'files_name': 'llama-3-multi_truth_judge', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 multillama3-truth 	 0.75	0.56	0.67	0.5	0.64
Meta-Llama 	 multillama3-truth 	 0.68	0.7	0.78	0.62	0.73
gemma-2-27 	 multillama3-truth 	 0.41	0.52	0.62	0.49	0.51
Meta-Llama 	 multillama3-truth 	 -	-	-	-	-
Average per language		 	 0.61	0.59	0.69	0.54	0.63
Errors in the labels: 0

 {'name': 'multi-gemma9b-truth', 'files_name': 'gemma9b_multi_truth_judge', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 multi-gemma9b-truth 	 0.72	0.6	0.61	0.41	0.68
Meta-Llama 	 multi-gemma9b-truth 	 0.65	0.73	0.7	0.62	0.81
gemma-2-27 	 multi-gemma9b-truth 	 0.51	0.56	0.54	0.47	0.59
Meta-Llama 	 multi-gemma9b-truth 	 -	-	-	-	-
Average per language		 	 0.63	0.63	0.62	0.5	0.69
Errors in the labels: 0

 {'name': 'inst-gemma9b-truth', 'files_name': 'gemma9b_instruct_truth_judge', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 inst-gemma9b-truth 	 0.79	0.66	0.62	0.48	0.68
Meta-Llama 	 inst-gemma9b-truth 	 0.74	0.67	0.6	0.54	0.68
gemma-2-27 	 inst-gemma9b-truth 	 0.52	0.51	0.57	0.41	0.56
Meta-Llama 	 inst-gemma9b-truth 	 -	-	-	-	-
Average per language		 	 0.68	0.61	0.6	0.48	0.64
Errors in the labels: 0

 {'name': 'multi-inst-gemma9b-truth', 'files_name': 'multi_gemma9b_instruct_truth_judge', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 multi-inst-gemma9b-truth 	 0.81	0.66	0.72	0.55	0.68
Meta-Llama 	 multi-inst-gemma9b-truth 	 0.72	0.67	0.67	0.6	0.83
gemma-2-27 	 multi-inst-gemma9b-truth 	 0.54	0.6	0.68	0.52	0.62
Meta-Llama 	 multi-inst-gemma9b-truth 	 0.87	0.88	0.94	0.72	0.74
Average per language		 	 0.74	0.7	0.75	0.6	0.72
Errors in the labels: 0

Average per model:
hf-llama2-truth 0.54
hf-llama2-info 0.28
new-llama3-truth 0.59
new-llama3-info 0.2
llama3.1-truth 0.57
gemma9b	 0.59
llama3.1-instruct 0.61
multi-llama3.1-inst 0.68
multi-ll3.1-truth 0.62
multillama3-truth 0.61
multi-gemma9b-truth 0.62
inst-gemma9b-truth 0.6
multi-inst-gemma9b-truth 0.7
