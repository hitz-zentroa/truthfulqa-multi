
 {'name': 'hf-llama2-truth', 'files_name': 'truthfulqa-truth-judge-llama2-7B', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 hf-llama2-truth 	 0.71	0.62	0.43	0.09	0.48
Meta-Llama 	 hf-llama2-truth 	 0.73	0.63	0.63	0.25	0.59
gemma-2-27 	 hf-llama2-truth 	 0.51	0.43	0.5	0.24	0.49
Average per language		 	 0.65	0.56	0.52	0.19	0.52
Errors in the labels: 24

 {'name': 'hf-llama2-info', 'files_name': 'truthfulqa-info-judge-llama2-7B', 'label': 'info'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 hf-llama2-info 	 0.49	0.27	0.22	0.31	0.26
Meta-Llama 	 hf-llama2-info 	 yes	yes	-0.03	0.12	yes
gemma-2-27 	 hf-llama2-info 	 0.14	0.3	0.16	0.09	0.21
Average per language		 	 0.31	0.29	0.12	0.17	0.24
Errors in the labels: 0

 {'name': 'new-llama3-truth', 'files_name': 'llama3-1_7B_truth_judge_final', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 new-llama3-truth 	 0.76	0.7	0.61	0.3	0.64
Meta-Llama 	 new-llama3-truth 	 0.67	0.64	0.66	0.46	0.58
gemma-2-27 	 new-llama3-truth 	 0.6	0.47	0.62	0.55	0.55
Average per language		 	 0.68	0.6	0.63	0.44	0.59
Errors in the labels: 0

 {'name': 'new-llama3-info', 'files_name': 'llama-3-info-new', 'label': 'info'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 new-llama3-info 	 0.66	yes	0.32	0.11	0.18
Meta-Llama 	 new-llama3-info 	 yes	yes	yes	0.15	-0.02
gemma-2-27 	 new-llama3-info 	 0.17	0.15	0.27	0.22	yes
Average per language		 	 0.41	0.15	0.3	0.16	0.08
Errors in the labels: 1

 {'name': 'llama3.1-truth', 'files_name': 'llama3.1.2_truth_judge', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 llama3.1-truth 	 0.77	0.63	0.54	0.31	0.52
Meta-Llama 	 llama3.1-truth 	 0.62	0.67	0.71	0.36	0.66
gemma-2-27 	 llama3.1-truth 	 0.55	0.51	0.6	0.47	0.58
Average per language		 	 0.65	0.6	0.62	0.38	0.58
Errors in the labels: 0

 {'name': 'multi-ll3.1-truth', 'files_name': 'llama3.1_multi_truth_judge', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 multi-ll3.1-truth 	 0.75	0.61	0.64	0.52	0.66
Meta-Llama 	 multi-ll3.1-truth 	 0.68	0.64	0.7	0.6	0.73
gemma-2-27 	 multi-ll3.1-truth 	 0.53	0.54	0.62	0.47	0.53
Average per language		 	 0.65	0.6	0.65	0.53	0.64
Errors in the labels: 0

 {'name': 'gemma9b\t', 'files_name': 'gemma9b_truth_judge', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 gemma9b	 	 0.79	0.62	0.67	0.41	0.58
Meta-Llama 	 gemma9b	 	 0.68	0.66	0.69	0.54	0.64
gemma-2-27 	 gemma9b	 	 0.46	0.55	0.58	0.39	0.54
Average per language		 	 0.65	0.61	0.65	0.45	0.59
Errors in the labels: 0

 {'name': 'llama3.1-instruct', 'files_name': 'llama3.1_instruct_truth_judge', 'label': 'truth'}
model_name 	 judge_name 		 en	es	ca	eu	gl
Meta-Llama 	 llama3.1-instruct 	 0.77	yes	yes	yes	yes
Meta-Llama 	 llama3.1-instruct 	 0.77	yes	yes	yes	yes
gemma-2-27 	 llama3.1-instruct 	 yes	yes	yes	yes	yes
Average per language		 	 0.77	-
Errors in the labels: 0

Average per model:
hf-llama2-truth 0.49
hf-llama2-info 0.21
new-llama3-truth 0.59
new-llama3-info 0.22
llama3.1-truth 0.57
multi-ll3.1-truth 0.61
gemma9b	 0.59
llama3.1-instruct 0.77
